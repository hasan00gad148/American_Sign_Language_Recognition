{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade3beb4-2c42-4e75-b0c0-c032b1218723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, ReLU, Dropout, GRU, Bidirectional\n",
    "#, ConvLSTM2D, Conv3D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random \n",
    "from sklearn.model_selection import train_test_split\n",
    "import mediapipe as mp\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f866c649-f1ac-4a9f-a4eb-0462ca49dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"../TrainData/\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "files = file_list[0:32]\n",
    "words = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1965ee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'book', 'call', 'car', 'day', 'egypt', 'english', 'enjoy',\n",
       "       'every', 'excuse', 'football', 'forget', 'fun', 'good', 'hate',\n",
       "       'have', 'hello', 'help', 'holiday', 'iam', 'love', 'meet', 'month',\n",
       "       'morning', 'my'], dtype='<U8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64aa8e11-f30b-435e-9555-49af1d38b446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 0,\n",
       " 'book': 1,\n",
       " 'call': 2,\n",
       " 'car': 3,\n",
       " 'day': 4,\n",
       " 'egypt': 5,\n",
       " 'english': 6,\n",
       " 'enjoy': 7,\n",
       " 'every': 8,\n",
       " 'excuse': 9,\n",
       " 'football': 10,\n",
       " 'forget': 11,\n",
       " 'fun': 12,\n",
       " 'good': 13,\n",
       " 'hate': 14,\n",
       " 'have': 15,\n",
       " 'hello': 16,\n",
       " 'help': 17,\n",
       " 'holiday': 18,\n",
       " 'iam': 19,\n",
       " 'love': 20,\n",
       " 'meet': 21,\n",
       " 'month': 22,\n",
       " 'morning': 23,\n",
       " 'my': 24}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formLableToNum = {label:num for num, label in enumerate(words)}\n",
    "formNumTOLabel = {num:label for num, label in enumerate(words)}\n",
    "formLableToNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbec8178-acdf-4d19-8ef3-787896f592d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 63\n",
      "book 59\n",
      "call 65\n",
      "car 61\n",
      "day 60\n",
      "egypt 65\n",
      "english 64\n",
      "enjoy 63\n",
      "every 65\n",
      "excuse 68\n",
      "football 58\n",
      "forget 66\n",
      "fun 69\n",
      "good 70\n",
      "hate 59\n",
      "have 59\n",
      "hello 68\n",
      "help 72\n",
      "holiday 59\n",
      "iam 62\n",
      "love 60\n",
      "meet 71\n",
      "month 62\n",
      "morning 81\n",
      "my 62\n"
     ]
    }
   ],
   "source": [
    "vids=[]\n",
    "labels=[]\n",
    "vidsnum = 0\n",
    "for w in words:\n",
    "    vidpath = path+w\n",
    "    vidlist = os.listdir(vidpath)\n",
    "    print(w,str(len(vidlist)))\n",
    "    for i in vidlist:\n",
    "        vid = []\n",
    "        labels.append(formLableToNum[w])\n",
    "        vidsnum+=1\n",
    "        for frame_num in range(20):\n",
    "            arr = np.load(os.path.join(path, w, i, \"{}.npy\".format(frame_num)))\n",
    "            vid.append(arr)\n",
    "        vid = np.reshape(np.array(vid),(20,258))\n",
    "        # m = vid.mean(axis=0)\n",
    "        # vid -=  m\n",
    "        m = vid.mean(axis=-1)\n",
    "        tmpv = vid.T \n",
    "        tmpv -=  m\n",
    "        v = tmpv.T\n",
    "        vids.append(v)\n",
    "        \n",
    "    \n",
    "labels = np.reshape(np.array(labels),(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d7dc4e6-56b1-4bb2-940f-740a3659b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = np.reshape(np.array(vids),(vidsnum,20,258))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d9d022-aa27-4eab-8f04-2ea40eafbcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1288, 20, 258), (1288, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(vids, labels, test_size = 0.2,shuffle=True,random_state=10)\n",
    "X_train.shape,y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86c0fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vids\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e21c6f6b-7df1-4290-b2a3-58059f0787e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_1 = Sequential([\n",
    "\n",
    "tf.keras.Input(shape=(20,258),),\n",
    "    \n",
    "    \n",
    "# GRU(64, return_sequences=True, activation='tanh', recurrent_dropout=0),\n",
    "# Dropout(0.2, noise_shape =  (1,20, 1)),\n",
    "\n",
    "# Bidirectional(LSTM(32, return_sequences=True, activation='relu'),input_shape=(20,258)),#, recurrent_dropout=0.2\n",
    "# Dropout(0.4, noise_shape =  (1,20, 1)),\n",
    "LSTM(32, return_sequences=True, activation='tanh'),\n",
    "LSTM(64, return_sequences=True, activation='tanh'),       \n",
    "LSTM(256, return_sequences=False, activation='tanh'),\n",
    "\n",
    "BatchNormalization(),\n",
    "Dropout(0.6),\n",
    "\n",
    "\n",
    "Dense(128, activation='linear',kernel_regularizer=tf.keras.regularizers.L1L2(0.01,2)),#kernel_regularizer=tf.keras.regularizers.L2(0)\n",
    "BatchNormalization(),#axis=-1,center=True,scale=True,\n",
    "ReLU(), \n",
    "\n",
    "Dense(64, activation='linear',kernel_regularizer=tf.keras.regularizers.L1L2(0.05,8)),#kernel_regularizer=tf.keras.regularizers.L2(0)\n",
    "BatchNormalization(),#axis=-1,center=True,scale=True,\n",
    "ReLU(),    \n",
    "\n",
    "\n",
    "\n",
    "Dense(25, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6675611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_47 (LSTM)              (None, 20, 32)            37248     \n",
      "                                                                 \n",
      " lstm_48 (LSTM)              (None, 20, 64)            24832     \n",
      "                                                                 \n",
      " lstm_49 (LSTM)              (None, 256)               328704    \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_32 (ReLU)             (None, 128)               0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_33 (ReLU)             (None, 64)                0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 25)                1625      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 435,353\n",
      "Trainable params: 434,457\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1b705f73-4201-44a9-b6b3-83899fe0ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_1.compile(tf.keras.optimizers.Adam(learning_rate=5e-4,beta_1=0.9,beta_2=0.999,epsilon=1e-07,)\n",
    "        ,loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "            ,metrics=[\"accuracy\"],)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "19121551-b92d-43af-8098-f674532e15a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/512\n",
      "11/11 [==============================] - 13s 385ms/step - loss: 1032.6439 - accuracy: 0.0598\n",
      "Epoch 2/512\n",
      "11/11 [==============================] - 4s 390ms/step - loss: 927.8524 - accuracy: 0.0738\n",
      "Epoch 3/512\n",
      "11/11 [==============================] - 4s 384ms/step - loss: 832.2668 - accuracy: 0.1064\n",
      "Epoch 4/512\n",
      "11/11 [==============================] - 4s 383ms/step - loss: 745.6214 - accuracy: 0.1359\n",
      "Epoch 5/512\n",
      "11/11 [==============================] - 4s 404ms/step - loss: 667.4821 - accuracy: 0.1786\n",
      "Epoch 6/512\n",
      "11/11 [==============================] - 5s 413ms/step - loss: 597.1599 - accuracy: 0.1879\n",
      "Epoch 7/512\n",
      "11/11 [==============================] - 4s 361ms/step - loss: 533.8422 - accuracy: 0.2189\n",
      "Epoch 8/512\n",
      "11/11 [==============================] - 4s 346ms/step - loss: 477.0205 - accuracy: 0.2275\n",
      "Epoch 9/512\n",
      "11/11 [==============================] - 4s 397ms/step - loss: 425.9354 - accuracy: 0.2578\n",
      "Epoch 10/512\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 380.0109 - accuracy: 0.2710\n",
      "Epoch 11/512\n",
      "11/11 [==============================] - 4s 328ms/step - loss: 338.7850 - accuracy: 0.3238\n",
      "Epoch 12/512\n",
      "11/11 [==============================] - 5s 465ms/step - loss: 301.8317 - accuracy: 0.3113\n",
      "Epoch 13/512\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 268.7150 - accuracy: 0.3168\n",
      "Epoch 14/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 239.0032 - accuracy: 0.3494\n",
      "Epoch 15/512\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 212.4287 - accuracy: 0.3750\n",
      "Epoch 16/512\n",
      "11/11 [==============================] - 4s 388ms/step - loss: 188.6780 - accuracy: 0.3634\n",
      "Epoch 17/512\n",
      "11/11 [==============================] - 5s 421ms/step - loss: 167.4259 - accuracy: 0.3851\n",
      "Epoch 18/512\n",
      "11/11 [==============================] - 4s 397ms/step - loss: 148.4326 - accuracy: 0.4231\n",
      "Epoch 19/512\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 131.4885 - accuracy: 0.4464\n",
      "Epoch 20/512\n",
      "11/11 [==============================] - 5s 468ms/step - loss: 116.3661 - accuracy: 0.4736\n",
      "Epoch 21/512\n",
      "11/11 [==============================] - 5s 435ms/step - loss: 102.9276 - accuracy: 0.4829\n",
      "Epoch 22/512\n",
      "11/11 [==============================] - 5s 412ms/step - loss: 90.9677 - accuracy: 0.4876\n",
      "Epoch 23/512\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 80.3419 - accuracy: 0.5016\n",
      "Epoch 24/512\n",
      "11/11 [==============================] - 4s 395ms/step - loss: 70.9345 - accuracy: 0.4868\n",
      "Epoch 25/512\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 62.5269 - accuracy: 0.5497\n",
      "Epoch 26/512\n",
      "11/11 [==============================] - 4s 394ms/step - loss: 55.0598 - accuracy: 0.5644\n",
      "Epoch 27/512\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 48.4619 - accuracy: 0.6126\n",
      "Epoch 28/512\n",
      "11/11 [==============================] - 4s 401ms/step - loss: 42.6714 - accuracy: 0.5994\n",
      "Epoch 29/512\n",
      "11/11 [==============================] - 4s 397ms/step - loss: 37.5258 - accuracy: 0.6320\n",
      "Epoch 30/512\n",
      "11/11 [==============================] - 5s 415ms/step - loss: 32.9676 - accuracy: 0.6421\n",
      "Epoch 31/512\n",
      "11/11 [==============================] - 4s 367ms/step - loss: 29.0027 - accuracy: 0.6506\n",
      "Epoch 32/512\n",
      "11/11 [==============================] - 5s 430ms/step - loss: 25.5104 - accuracy: 0.6615\n",
      "Epoch 33/512\n",
      "11/11 [==============================] - 5s 430ms/step - loss: 22.3624 - accuracy: 0.7003\n",
      "Epoch 34/512\n",
      "11/11 [==============================] - 4s 402ms/step - loss: 19.6131 - accuracy: 0.7003\n",
      "Epoch 35/512\n",
      "11/11 [==============================] - 5s 497ms/step - loss: 17.2613 - accuracy: 0.6949\n",
      "Epoch 36/512\n",
      "11/11 [==============================] - 6s 533ms/step - loss: 15.1923 - accuracy: 0.7267\n",
      "Epoch 37/512\n",
      "11/11 [==============================] - 6s 549ms/step - loss: 13.4362 - accuracy: 0.7143\n",
      "Epoch 38/512\n",
      "11/11 [==============================] - 7s 649ms/step - loss: 11.8203 - accuracy: 0.7376\n",
      "Epoch 39/512\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 10.4127 - accuracy: 0.7314\n",
      "Epoch 40/512\n",
      "11/11 [==============================] - 6s 482ms/step - loss: 9.1981 - accuracy: 0.7352\n",
      "Epoch 41/512\n",
      "11/11 [==============================] - 4s 354ms/step - loss: 8.2522 - accuracy: 0.7422\n",
      "Epoch 42/512\n",
      "11/11 [==============================] - 5s 490ms/step - loss: 7.3271 - accuracy: 0.7469\n",
      "Epoch 43/512\n",
      "11/11 [==============================] - 4s 369ms/step - loss: 6.5022 - accuracy: 0.7694\n",
      "Epoch 44/512\n",
      "11/11 [==============================] - 4s 349ms/step - loss: 5.6860 - accuracy: 0.8020\n",
      "Epoch 45/512\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 4.9880 - accuracy: 0.8043\n",
      "Epoch 46/512\n",
      "11/11 [==============================] - 4s 366ms/step - loss: 4.4709 - accuracy: 0.8137\n",
      "Epoch 47/512\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 4.1005 - accuracy: 0.8020\n",
      "Epoch 48/512\n",
      "11/11 [==============================] - 4s 358ms/step - loss: 3.7527 - accuracy: 0.8098\n",
      "Epoch 49/512\n",
      "11/11 [==============================] - 4s 342ms/step - loss: 3.4084 - accuracy: 0.8276\n",
      "Epoch 50/512\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 3.0769 - accuracy: 0.8331\n",
      "Epoch 51/512\n",
      "11/11 [==============================] - 4s 358ms/step - loss: 2.8828 - accuracy: 0.8183\n",
      "Epoch 52/512\n",
      "11/11 [==============================] - 4s 392ms/step - loss: 2.6854 - accuracy: 0.8238\n",
      "Epoch 53/512\n",
      "11/11 [==============================] - 4s 368ms/step - loss: 2.4989 - accuracy: 0.8191\n",
      "Epoch 54/512\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 2.2549 - accuracy: 0.8370\n",
      "Epoch 55/512\n",
      "11/11 [==============================] - 4s 385ms/step - loss: 2.0673 - accuracy: 0.8556\n",
      "Epoch 56/512\n",
      "11/11 [==============================] - 4s 342ms/step - loss: 2.0180 - accuracy: 0.8160\n",
      "Epoch 57/512\n",
      "11/11 [==============================] - 5s 449ms/step - loss: 1.8518 - accuracy: 0.8874\n",
      "Epoch 58/512\n",
      "11/11 [==============================] - 4s 348ms/step - loss: 1.7352 - accuracy: 0.8742\n",
      "Epoch 59/512\n",
      "11/11 [==============================] - 5s 439ms/step - loss: 1.7654 - accuracy: 0.8595\n",
      "Epoch 60/512\n",
      "11/11 [==============================] - 4s 358ms/step - loss: 1.9253 - accuracy: 0.8238\n",
      "Epoch 61/512\n",
      "11/11 [==============================] - 4s 397ms/step - loss: 1.8915 - accuracy: 0.8377\n",
      "Epoch 62/512\n",
      "11/11 [==============================] - 5s 408ms/step - loss: 1.9794 - accuracy: 0.7919\n",
      "Epoch 63/512\n",
      "11/11 [==============================] - 4s 361ms/step - loss: 1.8183 - accuracy: 0.8362\n",
      "Epoch 64/512\n",
      "11/11 [==============================] - 5s 434ms/step - loss: 1.8035 - accuracy: 0.8253\n",
      "Epoch 65/512\n",
      "11/11 [==============================] - 4s 347ms/step - loss: 1.6359 - accuracy: 0.8595\n",
      "Epoch 66/512\n",
      "11/11 [==============================] - 5s 449ms/step - loss: 1.5173 - accuracy: 0.8812\n",
      "Epoch 67/512\n",
      "11/11 [==============================] - 4s 355ms/step - loss: 1.4567 - accuracy: 0.8797\n",
      "Epoch 68/512\n",
      "11/11 [==============================] - 5s 420ms/step - loss: 1.3879 - accuracy: 0.8921\n",
      "Epoch 69/512\n",
      "11/11 [==============================] - 4s 383ms/step - loss: 1.3717 - accuracy: 0.8804\n",
      "Epoch 70/512\n",
      "11/11 [==============================] - 4s 357ms/step - loss: 1.5820 - accuracy: 0.8152\n",
      "Epoch 71/512\n",
      "11/11 [==============================] - 5s 442ms/step - loss: 1.5839 - accuracy: 0.8533\n",
      "Epoch 72/512\n",
      "11/11 [==============================] - 4s 342ms/step - loss: 1.5929 - accuracy: 0.8525\n",
      "Epoch 73/512\n",
      "11/11 [==============================] - 5s 488ms/step - loss: 1.4880 - accuracy: 0.8797\n",
      "Epoch 74/512\n",
      "11/11 [==============================] - 5s 399ms/step - loss: 1.4164 - accuracy: 0.8680\n",
      "Epoch 75/512\n",
      "11/11 [==============================] - 6s 527ms/step - loss: 1.4232 - accuracy: 0.8610\n",
      "Epoch 76/512\n",
      "11/11 [==============================] - 5s 399ms/step - loss: 1.4875 - accuracy: 0.8533\n",
      "Epoch 77/512\n",
      "11/11 [==============================] - 5s 489ms/step - loss: 1.4290 - accuracy: 0.8898\n",
      "Epoch 78/512\n",
      "11/11 [==============================] - 4s 362ms/step - loss: 1.4298 - accuracy: 0.9037\n",
      "Epoch 79/512\n",
      "11/11 [==============================] - 4s 392ms/step - loss: 1.3251 - accuracy: 0.9231\n",
      "Epoch 80/512\n",
      "11/11 [==============================] - 4s 403ms/step - loss: 1.2459 - accuracy: 0.9123\n",
      "Epoch 81/512\n",
      "11/11 [==============================] - 4s 362ms/step - loss: 1.3648 - accuracy: 0.8967\n",
      "Epoch 82/512\n",
      "11/11 [==============================] - 4s 407ms/step - loss: 1.4537 - accuracy: 0.8960\n",
      "Epoch 83/512\n",
      "11/11 [==============================] - 4s 363ms/step - loss: 1.4461 - accuracy: 0.8835\n",
      "Epoch 84/512\n",
      "11/11 [==============================] - 4s 378ms/step - loss: 1.3903 - accuracy: 0.8898\n",
      "Epoch 85/512\n",
      "11/11 [==============================] - 4s 386ms/step - loss: 1.3046 - accuracy: 0.8936\n",
      "Epoch 86/512\n",
      "11/11 [==============================] - 4s 394ms/step - loss: 1.2929 - accuracy: 0.9037\n",
      "Epoch 87/512\n",
      "11/11 [==============================] - 5s 431ms/step - loss: 1.3039 - accuracy: 0.8960\n",
      "Epoch 88/512\n",
      "11/11 [==============================] - 4s 370ms/step - loss: 1.2773 - accuracy: 0.9006\n",
      "Epoch 89/512\n",
      "11/11 [==============================] - 4s 408ms/step - loss: 1.1763 - accuracy: 0.9076\n",
      "Epoch 90/512\n",
      "11/11 [==============================] - 4s 344ms/step - loss: 1.0523 - accuracy: 0.9394\n",
      "Epoch 91/512\n",
      "11/11 [==============================] - 5s 428ms/step - loss: 1.3959 - accuracy: 0.8238\n",
      "Epoch 92/512\n",
      "11/11 [==============================] - 4s 388ms/step - loss: 1.4451 - accuracy: 0.8991\n",
      "Epoch 93/512\n",
      "11/11 [==============================] - 4s 403ms/step - loss: 1.4033 - accuracy: 0.8991\n",
      "Epoch 94/512\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 1.3071 - accuracy: 0.9185\n",
      "Epoch 95/512\n",
      "11/11 [==============================] - 4s 391ms/step - loss: 1.2186 - accuracy: 0.9309\n",
      "Epoch 96/512\n",
      "11/11 [==============================] - 5s 423ms/step - loss: 1.1379 - accuracy: 0.9317\n",
      "Epoch 97/512\n",
      "11/11 [==============================] - 4s 331ms/step - loss: 1.1616 - accuracy: 0.9146\n",
      "Epoch 98/512\n",
      "11/11 [==============================] - 5s 426ms/step - loss: 1.2588 - accuracy: 0.8929\n",
      "Epoch 99/512\n",
      "11/11 [==============================] - 4s 371ms/step - loss: 1.1834 - accuracy: 0.9278\n",
      "Epoch 100/512\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 1.0732 - accuracy: 0.9495\n",
      "Epoch 101/512\n",
      "11/11 [==============================] - 4s 378ms/step - loss: 1.1718 - accuracy: 0.9115\n",
      "Epoch 102/512\n",
      "11/11 [==============================] - 4s 388ms/step - loss: 1.2187 - accuracy: 0.9247\n",
      "Epoch 103/512\n",
      "11/11 [==============================] - 5s 418ms/step - loss: 1.1768 - accuracy: 0.9394\n",
      "Epoch 104/512\n",
      "11/11 [==============================] - 4s 353ms/step - loss: 1.4377 - accuracy: 0.8579\n",
      "Epoch 105/512\n",
      "11/11 [==============================] - 4s 387ms/step - loss: 1.3582 - accuracy: 0.9053\n",
      "Epoch 106/512\n",
      "11/11 [==============================] - 4s 382ms/step - loss: 1.2428 - accuracy: 0.9231\n",
      "Epoch 107/512\n",
      "11/11 [==============================] - 4s 387ms/step - loss: 1.0778 - accuracy: 0.9371\n",
      "Epoch 108/512\n",
      "11/11 [==============================] - 4s 387ms/step - loss: 1.0041 - accuracy: 0.9418\n",
      "Epoch 109/512\n",
      "11/11 [==============================] - 4s 371ms/step - loss: 1.0242 - accuracy: 0.9379\n",
      "Epoch 110/512\n",
      "11/11 [==============================] - 5s 411ms/step - loss: 0.9289 - accuracy: 0.9658\n",
      "Epoch 111/512\n",
      "11/11 [==============================] - 4s 376ms/step - loss: 0.9042 - accuracy: 0.9627\n",
      "Epoch 112/512\n",
      "11/11 [==============================] - 5s 479ms/step - loss: 1.1061 - accuracy: 0.9200\n",
      "Epoch 113/512\n",
      "11/11 [==============================] - 5s 414ms/step - loss: 1.0605 - accuracy: 0.9495\n",
      "Epoch 114/512\n",
      "11/11 [==============================] - 5s 474ms/step - loss: 1.2899 - accuracy: 0.8936\n",
      "Epoch 115/512\n",
      "11/11 [==============================] - 4s 397ms/step - loss: 1.2660 - accuracy: 0.9208\n",
      "Epoch 116/512\n",
      "11/11 [==============================] - 5s 423ms/step - loss: 1.0918 - accuracy: 0.9550\n",
      "Epoch 117/512\n",
      "11/11 [==============================] - 4s 356ms/step - loss: 1.1696 - accuracy: 0.9006\n",
      "Epoch 118/512\n",
      "11/11 [==============================] - 4s 386ms/step - loss: 1.1988 - accuracy: 0.9348\n",
      "Epoch 119/512\n",
      "11/11 [==============================] - 4s 351ms/step - loss: 1.1524 - accuracy: 0.9573\n",
      "Epoch 120/512\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 1.1972 - accuracy: 0.9363\n",
      "Epoch 121/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 1.0069 - accuracy: 0.9612\n",
      "Epoch 122/512\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.9709 - accuracy: 0.9441\n",
      "Epoch 123/512\n",
      "11/11 [==============================] - 4s 354ms/step - loss: 0.9776 - accuracy: 0.9480\n",
      "Epoch 124/512\n",
      "11/11 [==============================] - 4s 351ms/step - loss: 0.9707 - accuracy: 0.9550\n",
      "Epoch 125/512\n",
      "11/11 [==============================] - 4s 344ms/step - loss: 0.8796 - accuracy: 0.9713\n",
      "Epoch 126/512\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.8303 - accuracy: 0.9759\n",
      "Epoch 127/512\n",
      "11/11 [==============================] - 4s 337ms/step - loss: 0.7646 - accuracy: 0.9720\n",
      "Epoch 128/512\n",
      "11/11 [==============================] - 4s 329ms/step - loss: 0.7605 - accuracy: 0.9635\n",
      "Epoch 129/512\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.8580 - accuracy: 0.9627\n",
      "Epoch 130/512\n",
      "11/11 [==============================] - 4s 402ms/step - loss: 1.2169 - accuracy: 0.8944\n",
      "Epoch 131/512\n",
      "11/11 [==============================] - 4s 382ms/step - loss: 1.2198 - accuracy: 0.9402\n",
      "Epoch 132/512\n",
      "11/11 [==============================] - 4s 384ms/step - loss: 1.0944 - accuracy: 0.9596\n",
      "Epoch 133/512\n",
      "11/11 [==============================] - 4s 370ms/step - loss: 0.9830 - accuracy: 0.9425\n",
      "Epoch 134/512\n",
      "11/11 [==============================] - 5s 450ms/step - loss: 1.0322 - accuracy: 0.9154\n",
      "Epoch 135/512\n",
      "11/11 [==============================] - 4s 355ms/step - loss: 1.1602 - accuracy: 0.9177\n",
      "Epoch 136/512\n",
      "11/11 [==============================] - 4s 366ms/step - loss: 1.1234 - accuracy: 0.9589\n",
      "Epoch 137/512\n",
      "11/11 [==============================] - 4s 403ms/step - loss: 0.9886 - accuracy: 0.9728\n",
      "Epoch 138/512\n",
      "11/11 [==============================] - 5s 436ms/step - loss: 0.9230 - accuracy: 0.9643\n",
      "Epoch 139/512\n",
      "11/11 [==============================] - 5s 452ms/step - loss: 0.9235 - accuracy: 0.9589\n",
      "Epoch 140/512\n",
      "11/11 [==============================] - 5s 446ms/step - loss: 1.1307 - accuracy: 0.9371\n",
      "Epoch 141/512\n",
      "11/11 [==============================] - 5s 453ms/step - loss: 1.1489 - accuracy: 0.9627\n",
      "Epoch 142/512\n",
      "11/11 [==============================] - 5s 449ms/step - loss: 0.9487 - accuracy: 0.9821\n",
      "Epoch 143/512\n",
      "11/11 [==============================] - 5s 451ms/step - loss: 0.7116 - accuracy: 0.9884\n",
      "Epoch 144/512\n",
      "11/11 [==============================] - 6s 544ms/step - loss: 0.8121 - accuracy: 0.9526\n",
      "Epoch 145/512\n",
      "11/11 [==============================] - 6s 506ms/step - loss: 0.8246 - accuracy: 0.9752\n",
      "Epoch 146/512\n",
      "11/11 [==============================] - 5s 465ms/step - loss: 0.7296 - accuracy: 0.9767\n",
      "Epoch 147/512\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 0.5782 - accuracy: 0.9868\n",
      "Epoch 148/512\n",
      "11/11 [==============================] - 4s 368ms/step - loss: 0.5588 - accuracy: 0.9798\n",
      "Epoch 149/512\n",
      "11/11 [==============================] - 5s 416ms/step - loss: 0.6328 - accuracy: 0.9728\n",
      "Epoch 150/512\n",
      "11/11 [==============================] - 7s 633ms/step - loss: 0.6871 - accuracy: 0.9728\n",
      "Epoch 151/512\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 0.6890 - accuracy: 0.9775\n",
      "Epoch 152/512\n",
      "11/11 [==============================] - 4s 350ms/step - loss: 0.7249 - accuracy: 0.9821\n",
      "Epoch 153/512\n",
      "11/11 [==============================] - 4s 364ms/step - loss: 0.7549 - accuracy: 0.9814\n",
      "Epoch 154/512\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.6978 - accuracy: 0.9868\n",
      "Epoch 155/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.6099 - accuracy: 0.9806\n",
      "Epoch 156/512\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.6764 - accuracy: 0.9674\n",
      "Epoch 157/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.9966 - accuracy: 0.9488\n",
      "Epoch 158/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 1.0494 - accuracy: 0.9783\n",
      "Epoch 159/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 1.0818 - accuracy: 0.9449\n",
      "Epoch 160/512\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 1.4044 - accuracy: 0.8509\n",
      "Epoch 161/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 1.4125 - accuracy: 0.8812\n",
      "Epoch 162/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 1.2047 - accuracy: 0.9293\n",
      "Epoch 163/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 1.4312 - accuracy: 0.8447\n",
      "Epoch 164/512\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 1.5247 - accuracy: 0.8905\n",
      "Epoch 165/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 1.5560 - accuracy: 0.9200\n",
      "Epoch 166/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 1.4511 - accuracy: 0.9464\n",
      "Epoch 167/512\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 1.3866 - accuracy: 0.9418\n",
      "Epoch 168/512\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 1.1793 - accuracy: 0.9658\n",
      "Epoch 169/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 1.3843 - accuracy: 0.8703\n",
      "Epoch 170/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 1.4619 - accuracy: 0.9061\n",
      "Epoch 171/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 1.4158 - accuracy: 0.9565\n",
      "Epoch 172/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 1.2327 - accuracy: 0.9550\n",
      "Epoch 173/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.9266 - accuracy: 0.9821\n",
      "Epoch 174/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.6980 - accuracy: 0.9953\n",
      "Epoch 175/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.5435 - accuracy: 0.9946\n",
      "Epoch 176/512\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 0.6990 - accuracy: 0.9596\n",
      "Epoch 177/512\n",
      "11/11 [==============================] - 4s 319ms/step - loss: 1.0138 - accuracy: 0.9309\n",
      "Epoch 178/512\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 1.1009 - accuracy: 0.9666\n",
      "Epoch 179/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 1.1050 - accuracy: 0.9565\n",
      "Epoch 180/512\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 1.0560 - accuracy: 0.9697\n",
      "Epoch 181/512\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 0.9580 - accuracy: 0.9837\n",
      "Epoch 182/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.8151 - accuracy: 0.9845\n",
      "Epoch 183/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.7713 - accuracy: 0.9868\n",
      "Epoch 184/512\n",
      "11/11 [==============================] - 4s 320ms/step - loss: 0.7672 - accuracy: 0.9876\n",
      "Epoch 185/512\n",
      "11/11 [==============================] - 4s 326ms/step - loss: 0.9147 - accuracy: 0.9783\n",
      "Epoch 186/512\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 1.2210 - accuracy: 0.9666\n",
      "Epoch 187/512\n",
      "11/11 [==============================] - 5s 413ms/step - loss: 1.2169 - accuracy: 0.9899\n",
      "Epoch 188/512\n",
      "11/11 [==============================] - 4s 343ms/step - loss: 0.9543 - accuracy: 0.9891\n",
      "Epoch 189/512\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 0.7070 - accuracy: 0.9984\n",
      "Epoch 190/512\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.5336 - accuracy: 0.9992\n",
      "Epoch 191/512\n",
      "11/11 [==============================] - 4s 324ms/step - loss: 0.4890 - accuracy: 0.9969\n",
      "Epoch 192/512\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.4666 - accuracy: 0.9984\n",
      "Epoch 193/512\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.7484 - accuracy: 0.9697\n",
      "Epoch 194/512\n",
      "11/11 [==============================] - 4s 334ms/step - loss: 0.8690 - accuracy: 0.9884\n",
      "Epoch 195/512\n",
      "11/11 [==============================] - 5s 441ms/step - loss: 1.6609 - accuracy: 0.7609\n",
      "Epoch 196/512\n",
      "11/11 [==============================] - 4s 345ms/step - loss: 1.6830 - accuracy: 0.8517\n",
      "Epoch 197/512\n",
      "11/11 [==============================] - 4s 414ms/step - loss: 1.9172 - accuracy: 0.8672\n",
      "Epoch 198/512\n",
      "11/11 [==============================] - 4s 368ms/step - loss: 1.6861 - accuracy: 0.9278\n",
      "Epoch 199/512\n",
      "11/11 [==============================] - 4s 398ms/step - loss: 1.5992 - accuracy: 0.9208\n",
      "Epoch 200/512\n",
      "11/11 [==============================] - 5s 414ms/step - loss: 1.2855 - accuracy: 0.9519\n",
      "Epoch 201/512\n",
      "11/11 [==============================] - 5s 502ms/step - loss: 0.9503 - accuracy: 0.9728\n",
      "Epoch 202/512\n",
      "11/11 [==============================] - 7s 574ms/step - loss: 0.6804 - accuracy: 0.9899\n",
      "Epoch 203/512\n",
      "11/11 [==============================] - 7s 647ms/step - loss: 0.5557 - accuracy: 0.9884\n",
      "Epoch 204/512\n",
      "11/11 [==============================] - 5s 465ms/step - loss: 0.4604 - accuracy: 0.9961\n",
      "Epoch 205/512\n",
      "11/11 [==============================] - 7s 611ms/step - loss: 0.4636 - accuracy: 0.9891\n",
      "Epoch 206/512\n",
      "11/11 [==============================] - 6s 585ms/step - loss: 0.4727 - accuracy: 0.9953\n",
      "Epoch 207/512\n",
      "11/11 [==============================] - 6s 504ms/step - loss: 0.4773 - accuracy: 0.9899\n",
      "Epoch 208/512\n",
      "11/11 [==============================] - 7s 646ms/step - loss: 0.5439 - accuracy: 0.9868\n",
      "Epoch 209/512\n",
      "11/11 [==============================] - 6s 530ms/step - loss: 0.6739 - accuracy: 0.9899\n",
      "Epoch 210/512\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 0.7762 - accuracy: 0.9876\n",
      "Epoch 211/512\n",
      "11/11 [==============================] - 7s 633ms/step - loss: 0.7888 - accuracy: 0.9790\n",
      "Epoch 212/512\n",
      "11/11 [==============================] - 5s 443ms/step - loss: 0.8027 - accuracy: 0.9821\n",
      "Epoch 213/512\n",
      "11/11 [==============================] - 7s 638ms/step - loss: 0.8708 - accuracy: 0.9736\n",
      "Epoch 214/512\n",
      "11/11 [==============================] - 6s 554ms/step - loss: 0.7300 - accuracy: 0.9938\n",
      "Epoch 215/512\n",
      "11/11 [==============================] - 6s 509ms/step - loss: 0.6325 - accuracy: 0.9922\n",
      "Epoch 216/512\n",
      "11/11 [==============================] - 7s 659ms/step - loss: 0.5386 - accuracy: 0.9977\n",
      "Epoch 217/512\n",
      "11/11 [==============================] - 5s 489ms/step - loss: 0.4252 - accuracy: 0.9992\n",
      "Epoch 218/512\n",
      "11/11 [==============================] - 7s 572ms/step - loss: 0.4800 - accuracy: 0.9930\n",
      "Epoch 219/512\n",
      "11/11 [==============================] - 7s 614ms/step - loss: 0.5671 - accuracy: 0.9922\n",
      "Epoch 220/512\n",
      "11/11 [==============================] - 5s 418ms/step - loss: 0.5402 - accuracy: 0.9984\n",
      "Epoch 221/512\n",
      "11/11 [==============================] - 8s 746ms/step - loss: 0.4415 - accuracy: 1.0000\n",
      "Epoch 222/512\n",
      "11/11 [==============================] - 9s 803ms/step - loss: 0.3821 - accuracy: 0.9992\n",
      "Epoch 223/512\n",
      "11/11 [==============================] - 6s 570ms/step - loss: 0.3887 - accuracy: 0.9969\n",
      "Epoch 224/512\n",
      " 8/11 [====================>.........] - ETA: 2s - loss: 0.3575 - accuracy: 0.9990"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_lstm_1\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\u001b[39m#batch_size=64, validation_data=(X_dev, y_dev)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_lstm_1.fit(X_train, y_train, epochs=300, batch_size=128)#batch_size=64, validation_data=(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "22e72f3c-84c8-42a0-bc88-65a31541f8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 7s 100ms/step - loss: 0.3540 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3539767861366272, 1.0]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_1.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "20ff28bc-1d2c-48ae-a345-e8f57c9421f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 81ms/step - loss: 0.5968 - accuracy: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.596828818321228, 0.9411764740943909]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_1.evaluate(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f8bd5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4df36907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_54_layer_call_fn, lstm_cell_54_layer_call_and_return_conditional_losses, lstm_cell_55_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./modelAsl/v1_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./modelAsl/v1_\\assets\n"
     ]
    }
   ],
   "source": [
    "model_lstm_1.save(\"./modelAsl/v2_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec565530",
   "metadata": {},
   "source": [
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4025f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\"./modelAsl/v8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee478e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85176ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return  results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfd71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.25, min_tracking_confidence=0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c643c24-51c9-4bf9-9260-e55f2f5e4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoProc(path, c=5):\n",
    "    \"###\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    fno = 0\n",
    "    framecount = 20\n",
    "    \n",
    "    v = []\n",
    "    \n",
    "    skipCounter = c\n",
    "    framerateCounter= round(fps/20, 0) \n",
    "\n",
    "    while True and success:\n",
    "        success, frame = cap.read()\n",
    "        if fno >=framecount:\n",
    "            break \n",
    "        \n",
    "     \n",
    "        \n",
    "        if skipCounter>=0:\n",
    "            pass\n",
    "        else:\n",
    "            skipCounter-=1\n",
    "            continue\n",
    "        \n",
    "        if fpsCounter==0:\n",
    "            fpsCounter=framerateCounter-1\n",
    "        else:\n",
    "            fpsCounter-=1\n",
    "            continue\n",
    "\n",
    "        if success:\n",
    "            frame = cv2.resize(frame, (720,480), interpolation = cv2.INTER_AREA)\n",
    "            results = mediapipe_detection(frame, holistic)\n",
    "            eres = extract_keypoints(results)\n",
    "\n",
    "            tmp=  np.reshape(eres, (1,258))\n",
    "            v.append(tmp)\n",
    "            \n",
    "            # draw_styled_landmarks(frame, results)\n",
    "            # plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            # plt.show()\n",
    "            fno += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# if cv2.waitKey(1) & 0xFF == ord('q') or ret==False :\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    f = len(v)\n",
    "    v = np.array(v) if f >0 else[]\n",
    "    v =np.reshape(v,(f,258))if f >0 else[]\n",
    "   \n",
    "\n",
    "\n",
    "    if f>0:      \n",
    "        m = v.mean(axis=-1)     \n",
    "        tmpv = v.T \n",
    "        tmpv -=  m\n",
    "        v = tmpv.T\n",
    "        m = v.mean(axis=0) \n",
    "        v -= m\n",
    "    if fno<framecount and fno>0:\n",
    "        tmp = np.zeros((framecount-fno,258))\n",
    "        v = np.concatenate((v, tmp), axis=0) \n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = videoProc(\".\\\\TestDataDD\\\\z1.mp4\",20)\n",
    "# 1 0 3 2 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18522b4",
   "metadata": {},
   "source": [
    "(20) 16 (9) 7 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.min(),v.max(),v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82da8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = reconstructed_model.predict_step(np.reshape(v, (1,20,258)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049870bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p),p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20399fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8bfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c4f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba785eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b71ffd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af3319e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(25, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf73d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 20, 258)]    0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 20, 258)     516         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 20, 258)     1060098     ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 20, 258)      0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 20, 258)     0           ['dropout_30[0][0]',             \n",
      " ambda)                                                           'input_17[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 20, 4)        1036        ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 20, 4)        0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 20, 258)      1290        ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 20, 258)     0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 20, 258)     1060098     ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 20, 258)      0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 20, 258)     0           ['dropout_32[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 20, 4)        1036        ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 20, 4)        0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 20, 258)      1290        ['dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 20, 258)     0           ['conv1d_19[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 20, 258)     1060098     ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 20, 258)      0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 20, 258)     0           ['dropout_34[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 20, 4)        1036        ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 20, 4)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 20, 258)      1290        ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 20, 258)     0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 20, 258)     1060098     ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 20, 258)      0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 20, 258)     0           ['dropout_36[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 20, 258)     516         ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 20, 4)        1036        ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 20, 4)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 20, 258)      1290        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 20, 258)     0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 20)          0           ['tf.__operators__.add_23[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 128)          2688        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 128)          0           ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 25)           3225        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,259,737\n",
      "Trainable params: 4,259,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 29s 1s/step - loss: 3.2380 - sparse_categorical_accuracy: 0.0447 - val_loss: 3.2267 - val_sparse_categorical_accuracy: 0.0465\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 17s 1s/step - loss: 3.2181 - sparse_categorical_accuracy: 0.0563 - val_loss: 3.2167 - val_sparse_categorical_accuracy: 0.0426\n",
      "Epoch 3/200\n",
      " 7/17 [===========>..................] - ETA: 10s - loss: 3.1865 - sparse_categorical_accuracy: 0.0714"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     21\u001b[0m callbacks \u001b[39m=\u001b[39m [keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)]\n\u001b[1;32m---> 23\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     24\u001b[0m     X_train,\n\u001b[0;32m     25\u001b[0m     y_train,\n\u001b[0;32m     26\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     27\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[0;32m     28\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m     29\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Hassan\\Desktop\\ASL\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_1.evaluate(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ac56168275f2fd660385742ab2ada9d3df0956fd35a89dbc805e65b54dfb51c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
