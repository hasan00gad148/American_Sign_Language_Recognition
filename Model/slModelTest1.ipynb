{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb08840-5718-4359-ab15-8ad8412150b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## import libraries\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092833cf-f2a9-483a-8212-13949aebee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, ReLU, Dropout,Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random \n",
    "from sklearn.model_selection import train_test_split\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2a1a7-1955-4e42-afce-a040d6cc0f05",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbde8f-1dc7-4b40-97a6-34631506a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "tf.config.list_physical_devices('GPU')\n",
    "# tf.device('/gpu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c4f73-a2b8-4ccd-b45f-4720eedf68da",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## MobileNetV2_____START##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34baf94-d158-4ac7-82ea-9e376661b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803def23-4254-4f6d-9e72-18b44bff4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (160, 160, 3)\n",
    "MobileNetV2 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f4e28-3b24-4cd8-be7a-0d9a3c722ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MobileNetV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aabef36-a174-4028-914e-c4fa43fdad4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_layers = len(MobileNetV2.layers)\n",
    "print(MobileNetV2.layers[nb_layers - 2].name)\n",
    "print(MobileNetV2.layers[nb_layers - 1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbb4ca-f56f-4345-83d1-a2f3f13e851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = preprocess_input(inputs)\n",
    "x = MobileNetV2(x)\n",
    "\n",
    "x =tfl.GlobalAveragePooling2D()(x)\n",
    "model = tf.keras.Model(inputs,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ce7a1-fc83-49c7-804d-0c1ae0ad3b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca432c-b8fd-4cce-9b1a-09e7aae862c5",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26e7f4-31bf-487a-a306-f97753578168",
   "metadata": {},
   "source": [
    "***\n",
    "## load json into dataFrame_____Start###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43066bf3-2a1e-4a1a-a92f-9bfe19ea92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('F://aaaaaaaaaaa//ml_advanced_ng//Certificates//WLASL_v0.3.json', 'r') as f:\n",
    "  jdata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bc7cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14780\n",
      "69291\n",
      "14797\n",
      "14799\n",
      "14800\n",
      "14801\n",
      "65444\n",
      "14792\n",
      "67553\n",
      "14802\n",
      "14803\n",
      "14793\n",
      "14794\n",
      "14795\n",
      "14796\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(jdata[199]['instances'])):\n",
    "    \n",
    "    print(jdata[199]['instances'][i]['video_id'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cac4834-824b-4db1-92b2-ac73c00b416e",
   "metadata": {
    "tags": []
   },
   "source": [
    "jdata[0][\"gloss\"]\n",
    "jdata[0]['instances']\n",
    "len(jdata[0]['instances'])\n",
    "jdata[0]['instances'][0]\n",
    "jdata[0]['instances'][0]['video_id']\n",
    "jdata = jdata[50:128]\n",
    "len(jdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156f1ad-37c2-4017-9d05-35f2ee3c72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jdata = jdata[50:128]\n",
    "\n",
    "vidsIDS={}\n",
    "for i in range(10):\n",
    "    vidsIDS[jdata[i][\"gloss\"]] = []\n",
    "\n",
    "# print(len(jdata),len(vidsIDS))\n",
    "len(vidsIDS.keys())\n",
    "vidsIDS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329f952-4462-4cc4-aecb-0fe5022d8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlen = len(vidsIDS)\n",
    "k=0\n",
    "for i in vidsIDS.keys():\n",
    "    wordVidsLen = len(jdata[k]['instances'])\n",
    "    for j in range(wordVidsLen):\n",
    "        vidsIDS[i].append(jdata[k]['instances'][j]['video_id'])        \n",
    "    k+=1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a02785-088a-4064-a503-ff6b728ffdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del jdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0636e94-a560-4a10-bd3e-0bf228e42f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in vidsIDS.keys():\n",
    "    print(len(vidsIDS[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922e625-9014-4b4e-81a5-4ebfe5646589",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(vidsIDS.keys())\n",
    "data = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in vidsIDS.items() ]),columns =cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a899dca-d6c9-4fd0-9cd4-e7e8ddfc76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef86c8-7776-414c-beab-931779d6c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cols:\n",
    "    data.loc[:,i] = data.loc[:,i].apply(lambda x:\"F://aaaaaaaaaaa//ml_advanced_ng//Certificates//videos//\"+x+\".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac52fa-a592-4474-b8b1-c9a8249e4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0ca08-bb02-465e-8ecb-e450d092ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[2,0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d9b86-0a60-40a2-b2b3-b40630eab61c",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c384c-1639-4cd3-b560-42dfe3cf5e98",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## holistic________START###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a630bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92949d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59a617-538f-4bc4-86df-c9e5aeee6f38",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176aab2-8901-400f-881a-72b6b8a5a17c",
   "metadata": {},
   "source": [
    "***\n",
    "## video Processing & saving .npy files________START###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bcebf-c1cc-4a3c-9fcf-96f2e3a14a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoProc(path=data.iloc[2,1]):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    success, frame = cap.read()\n",
    "    fno = 0\n",
    "    framecount = 60\n",
    "    v = None\n",
    "    if success:\n",
    "        # img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # img = cv2.resize(img, (160,160), interpolation = cv2.INTER_AREA)\n",
    "        # v = model.predict_step(np.array([img]))\n",
    "        # height = frame.shape[0]\n",
    "        # width = frame.shape[1]\n",
    "        # print(width,height)\n",
    "        \n",
    "        img = frame\n",
    "        v =  np.reshape(extract_keypoints(mediapipe_detection(frame, holistic)), (1,-1))\n",
    "        # print(v.shape)\n",
    "        fno = 1\n",
    "        while True:\n",
    "            # Show to screen\n",
    "            # cv2.imshow('OpenCV Feed', frame):\n",
    "            if fno >=framecount:\n",
    "                break \n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if success:\n",
    "                # img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # v = np.concatenate((v, model.predict_step(np.array([img]))), axis=0) \n",
    "                \n",
    "                v = np.concatenate((v,np.reshape(extract_keypoints(mediapipe_detection(frame, holistic)), (1,-1))), axis=0) \n",
    "                fno += 1\n",
    "            else:\n",
    "                break\n",
    "            # Break \n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if fno<framecount and fno>0:\n",
    "        tmp = np.zeros((framecount-fno,1662)) \n",
    "        v = np.concatenate((v, tmp), axis=0) \n",
    "\n",
    "    return fno,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45200527-017b-4583-b9ac-ccbe37a72cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fno,vidarr = videoProc(path=data.iloc[1,5])\n",
    "vidarr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd119a9-1646-4ab1-8be2-b9fd06d89a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61722867-abcb-4a76-a27d-fa75d70ff12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vidno = 0\n",
    "_i=0\n",
    "for w in cols:\n",
    "    instantsLen = len(data.loc[:,w])\n",
    "    data.loc[:,w] = data.loc[:,w].apply(str)\n",
    "    print(w+\"___no#\"+str(_i))\n",
    "    _i+=1\n",
    "    for i in range(instantsLen):\n",
    "        s = str(data.loc[i,w])\n",
    "        if s.split(\"//\")[-1] == \"nan.mp4\":\n",
    "            # print(s)\n",
    "            break\n",
    "        fno,vidarr = videoProc(path=data.loc[i,w])\n",
    "\n",
    "        # print(fno)\n",
    "        if fno == 0:\n",
    "            continue\n",
    "\n",
    "        vidno += 1\n",
    "        # np.save('.//vidsNumpy//'+w+'__'+str(i)+'.npy', vidarr)        \n",
    "        np.save('.//vnh//'+w+'__'+str(i)+'.npy', vidarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d881af-3e4e-4551-adca-6e0ceccccb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91f98e-0a17-4335-a8bb-a042a65f5347",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee06a9-669b-43ba-8184-db8bb8aab409",
   "metadata": {},
   "source": [
    "***\n",
    "## use tf.data.dataset to create data pipe line_______START###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4997d-f097-45d3-982b-7dc31141fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy_file(item):\n",
    "    \n",
    "    data = np.load(item.numpy().decode())\n",
    "    return data\n",
    "#, label_map[item.numpy().decode().split('\\\\')[-1].split('__')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebc980-c6e6-4172-8297-855ce2496c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(\"./vnh\")\n",
    "random.shuffle(file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89426cb-1f49-4953-be47-26778cb46c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "formLableToNum = {label:num for num, label in enumerate(cols)}\n",
    "formNumTOLabel = {num:label for num, label in enumerate(cols)}\n",
    "formLableToNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b2546-5018-4b78-b4c3-efa063ee2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = [f.split('__')[0] for f in file_list]\n",
    "ln = list(np.unique(lables))\n",
    "len(ln)\n",
    "lables[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97fd48-b46f-4be6-997e-6bece42d5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = [formLableToNum[f.split('__')[0]] for f in file_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561477f-ee08-4ae4-8f89-a46ff325196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = tf.data.Dataset.from_tensor_slices(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84778416-0868-4d06-87f4-44b1f1bc68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\".\\\\vnh\\\\\"+f for f in file_list]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2261f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda item:tf.py_function(read_npy_file, [item], [tf.float32]))\n",
    "dataset = tf.data.Dataset.zip((dataset, label_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad84e6f-e00c-4b0d-885d-aef429f7aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset)*0.8)\n",
    "\n",
    "train_ds = dataset.take(train_size)\n",
    "test_ds = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e35cd-da64-45a7-9cf2-661b6e8cb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134674c5-da51-4589-b2e0-fb86ac7b266b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for n,l in dataset.take(5):\n",
    "    print(n[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce058c-ab26-47d6-b310-e85b6eeb3be0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a90c2-0e1f-4f79-9e8c-8f036c5d740b",
   "metadata": {},
   "source": [
    "***\n",
    "## create lstm model______START################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a3461-2cc2-46ee-967f-00935c18bb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_lstm_1 = Sequential([\n",
    "\n",
    "tf.keras.Input(shape=(40,1662),),\n",
    "\n",
    "Bidirectional(LSTM(128, return_sequences=True, activation='tanh'),input_shape=(40,1662)), \n",
    "Dropout(0.1, noise_shape = (1,80, 1)),\n",
    "    \n",
    "Bidirectional(LSTM(128, return_sequences=True, activation='tanh'),input_shape=(40,1662)), \n",
    "Dropout(0.1, noise_shape =  (1,80, 1)),\n",
    "\n",
    "LSTM(128, return_sequences=False, activation='tanh', recurrent_dropout=0),\n",
    "BatchNormalization(axis=-1,center=True,scale=True,),\n",
    "Dropout(0.1,),\n",
    "\n",
    "      \n",
    "# LSTM(128, return_sequences=False, activation='relu', recurrent_dropout=0.2),\n",
    "# BatchNormalization(axis=-1,center=True,scale=True,),\n",
    "# Dropout(0.1,),\n",
    "    \n",
    "Dense(128, activation='linear',kernel_regularizer=tf.keras.regularizers.L2(1)),\n",
    "BatchNormalization(axis=-1,center=True,scale=True,),\n",
    "ReLU(),\n",
    "    \n",
    "Dense(128, activation='linear',kernel_regularizer=tf.keras.regularizers.L1(0.1)),\n",
    "BatchNormalization(axis=-1,center=True,scale=True,),\n",
    "ReLU(),\n",
    "    \n",
    "Dense(30, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d749da-d160-4dc3-ba46-acc58b9f66d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_lstm_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,)\n",
    ",loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "            ,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_1.fit(train_ds.batch(32), epochs=256, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252020e2-f344-4d4a-bfba-7072e5a13d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_lstm_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d40f39-2388-4f2f-b20c-4e2d593c5f48",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6564787-4cd8-4b90-b61a-bd549b10b58c",
   "metadata": {},
   "source": [
    "***\n",
    "## test_______START############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cca172",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_l =[]\n",
    "train_ds_np = []\n",
    "for i,j in train_ds.take(1000):\n",
    "    train_ds_l.append(j.numpy())\n",
    "    train_ds_np.append(i[0].numpy())\n",
    "    \n",
    "train_ds_np = np.array(train_ds_np)\n",
    "train_ds_l = np.array(train_ds_l)\n",
    "train_ds_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model_lstm_1.predict(train_ds_np)\n",
    "a = np.sum((np.argmax(p,axis=1)==train_ds_l))/ p.shape[0]\n",
    "print(np.argmax(p[51]),np.argmax(train_ds_l[51]),a)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83634767-12e4-46fd-b7e0-6e311037b1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds_l =[]\n",
    "test_ds_np = []\n",
    "for i,j in test_ds:\n",
    "    test_ds_l.append(j.numpy())\n",
    "    test_ds_np.append(i[0].numpy())\n",
    "    \n",
    "test_ds_np = np.array(test_ds_np)\n",
    "test_ds_l = np.array(test_ds_l)\n",
    "test_ds_l.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29f1e5-f355-498a-a298-c2439d3bd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model_lstm_1.predict(test_ds_np)\n",
    "a = np.sum((np.argmax(p,axis=-1)==test_ds_l))/ p.shape[0]\n",
    "print(np.argmax(p[2]),np.argmax(test_ds_l[2]),a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44371b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f7bca-3131-4e33-b66e-ad248a1a5072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "26cdd06f4768c01aff35b8a9e3baf997ace62fbda00b5d467f0db2807f78448e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
